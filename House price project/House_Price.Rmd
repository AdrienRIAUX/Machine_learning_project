---
title: "House price project"
author : "Adrien Riaux"
date : "01/08/2022"
output: rmarkdown::github_document
---

# House price project

```{r}
#Import 
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(coefplot)
library(forecast)
```

We have several columns : 
- datesold : date of the property sale
- postcode : postal code of the property
- price : sale price of the property 
- propertyType : unit or house 
- bedrooms : number of bedrooms per property

```{r}
#Read data
df = read.csv("house_price.csv", header = TRUE, sep = ",")
head(df)
```
We can observe that our dataframe contains date, but there store as characters. Moreover, it seems that the propertyType and bedrooms columns are categorical features. To verify that, we observe the number of unique value for these features.

```{r}
#Check unique value of categorical features
for (i in c("propertyType", "bedrooms")) {
  print(unique(df[,i]))
}
```

Well, we have only to possible values for properyType and 6 for bedrooms. Use propertyType as factor would be interesting but we will keep bedrooms as int, because there are an order in this features. Have 3 bedrooms is better than only 2 for example.

So we can now convert propertyType and datesold features into appropriate types.

```{r}
#Convert type
df[, "propertyType"] <- as.factor(df[, "propertyType"])
df[, "datesold"] <- as.Date(df[, "datesold"])

sapply(df, class)
```

```{r}
#Data description
summary(df)
```

We have no null values in the dataframe. Moreover postcode is only 4 digits reference, we don't expect that this data will be useful in this form. We will probably transform it into a categorical class. 

Price has outliers, we will deal with it later. 

### Exploratory data analysis

```{r}
#Plot histogram of price
ggplot(data = df, aes(x = price)) +
  geom_histogram(alpha = 0.7)

#Plot histogram of price using log scale
ggplot(data = df, aes(x = log(price))) + 
  geom_histogram(alpha = 0.7) 
```
Price has a log normal distribution, then it can be difficult for our machine learning model to predict value. Using log on price give a normal distribution. So we create a new column : logPrice = log(price). 

```{r}
#Create logPrice column
df$logPrice = log(df$price)
head(df)
```


```{r}
#Plot density of postcode per propertyType
ggplot(data = df, aes(x = postcode)) + #Add histogram
  geom_histogram(aes(
    y = ..density.., 
    fill = propertyType, 
    color = propertyType
  ), 
  alpha = 0.3, position = "identity") + 
  geom_density(aes(color = propertyType), size = 0.7)
```
We have two group of postcode. But neither match with a propertyType. It could be interesting to create a categorical feature for the postcode and check if they are a place where the price is higher. It can be interpreted like neighbourhoods.

```{r}
#Create a categorical feature
df <- df %>% mutate(postcode = case_when(postcode > 2800 ~ "pc29",
                                         TRUE ~ "pc26"))

#Convert type
df[, "postcode"] <- as.factor(df[, "postcode"])
```

Now we can visualize price per propertyType and postcode. 

```{r}
#Price boxplot
ggplot(data = df, aes(x = propertyType, y = logPrice)) + geom_boxplot(aes(color = postcode))
```

It seems that property with postcode start with 26 are little more expensive than the others, but it still have an overlap between the two neighbourhoods.

Another interesting graphic is price against number of bedrooms. 

```{r}
#Price against bedrooms
ggplot(df, aes(x = factor(bedrooms), y = logPrice)) + 
  geom_boxplot(alpha = 0.3) +
  labs(x = "number of bedrooms")
```
Some houses/units with 0 bedrooms. It seems strange, moreover if we look at the boxplot, the price for these houses/units are particularly expensive.
So we check the number of values per bedrooms.

```{r}
#Value counts for the bedrooms feature
table(df$bedrooms)
```

They are only 30 houses/units with 0 bedrooms. We will simply drop them.

```{r}
#Drop line with zero bedrooms
print(nrow(df))
df <- df %>% filter(bedrooms != 0)
print(nrow(df))
```
Very good, we have drop lines with zero bedrooms. 

Another important graphic is the correlation heatmap. It shows the correlation between features, i.e. we need to test price and bedrooms relationship with each other.

```{r}
#Create a correlation matrix
corr_df <- df %>% select(c("logPrice", "bedrooms")) %>% cor()
corr_df
```


```{r}
#Create month and year columns
df$year <- year(df$datesold)
df$month <- month(df$datesold, label = TRUE)
```


```{r}
#Reshape data by year and month and calculate the price average
df_time <- df %>% group_by(year, month) %>% summarize(med_logPrice = median(logPrice))

#Plot price evolution per month
ggplot(df_time, aes(x = month, y = med_logPrice)) +
  geom_line(aes(color = factor(year), group = year)) +
  scale_color_discrete(name = "year")
  
```


```{r}
#Reshape data by year
df_time <- df %>% group_by(year, postcode) %>% summarize(med_logPrice = median(logPrice))

ggplot(df_time, aes(x = year, y = med_logPrice)) +
  geom_line(aes(color = postcode))
```

### Modeling 

We will define a linear model to predict the price with other features.

```{r}
#Fit the linear regression
model <- lm(logPrice ~ datesold + bedrooms + postcode + propertyType,
             data = df)

#Model informations
summary(model)
```

Now our linear regression are fit. We can use the summary function to print out information about the models.

We have a R-squared of 49%, it reveals that 49% of the variability observed in the target variable (logPrice) is explained by the regression model. 

The coefficients represents the effect of the predictors on the response (logPrice) and the standard errors are the uncertainty in the estimation of the coefficients. We a visualisation plot to show the coefficient of the regression model. In general, a good rule of thumb is that if the two standard error confidence interval does not contain 0, it is statistically significant.

```{r}
#Visualize model coefficient
coefplot(model, sort = 'mag')

#Visualize model coefficient with limited x axis scale
coefplot(model, sort = 'mag') + scale_x_continuous(limits = c(-0.5,0.5))
```

```{r}
#Check model performance with residual against fitted values
ggplot(aes(x = .fitted, y = .resid), data = model) +
  geom_point(aes(color = factor(bedrooms)), alpha = 0.7) +
  geom_hline(yintercept = 0) +
  geom_smooth(se = T) +
  labs(x = "Fitted values", y = "Residuals") +
  scale_color_discrete(name = "bedrooms")

```

```{r}
#Q-Q plot
ggplot(aes(sample = .stdresid), data = model) + 
  stat_qq() + 
  geom_abline() +
  labs(title = "Q-Q plot")
```

Now we are interesting in modelisation of time series. 

```{r}
#Define the new dataframe with the median log price per year
df_time <- df %>% group_by(year) %>% summarize(meanPrice = mean(price))

#We convert it to a time series
timePrice <- ts(df_time$meanPrice, start = min(df_time$year), end = max(df_time$year))

#Plot the time series
plot(timePrice)
```

```{r}
#Show the autocovariance function (ACF)
acf(timePrice)
```

```{r}
#Determining the optimal number of diffs
ndiffs(x = timePrice)

plot(diff(timePrice, 1))
```

```{r}
time_forcastPrice <- auto.arima(x = timePrice)
time_forcastPrice
```

```{r}
acf(time_forcastPrice$residuals)
```

```{r}
predict(time_forcastPrice, n.ahead = 5, se.fit = T)
```

```{r}
plot(forecast(object = time_forcastPrice, h = 5))
```

