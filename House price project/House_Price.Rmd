---
title: "House price project"
author : "Adrien Riaux"
date : "01/08/2022"
output: rmarkdown::github_document
---

# House price project

```{r}
#Import 
library(dplyr)
library(tidyr)
library(ggplot2)
library(coefplot)
```

We have several columns : 
- datesold : date of the property sale
- postcode : postal code of the property
- price : sale price of the property 
- propertyType : unit or house 
- bedrooms : number of bedrooms per property

```{r}
#Read data
df = read.csv("house_price.csv", header = TRUE, sep = ",")
head(df)
```
We can observe that our dataframe several informations. For these data the response is the value per square foot and the predictors are everything else.

Moreover, it seems that the Boro column is categorical features. To verify that, we observe the number of unique value for these features.


```{r}
#Check unique value of categorical features
print(unique(df[,"Boro"]))
```

Well, we have only five possible values for Boro column. Use Boro as factor would be interesting.

So we can now convert Boro feature into an appropriate type.

```{r}
#Convert type
df[, "Boro"] <- as.factor(df[, "Boro"])
sapply(df, class)
```

```{r}
#Data description
summary(df)
```

We have no null values in the dataframe. Regarding Units, Income, Expense and Value features we have outliers. It is important to identify them. We will deal with it later.

### Exploratory data analysis

```{r}
#Plot histogram of value per suqare foot
ggplot(data = df, aes(x = ValuePerSqFt)) +
  geom_histogram(alpha = 0.7)
```
We can see that the histogram has a bimodal nature. So we explore more using histogram with a mapping color to Boro feature.

```{r}
# Histogram using a mapping color to Boro
ggplot(data = df, aes(x = ValuePerSqFt, fill = Boro)) + 
  geom_histogram(alpha = 0.7)

# Same histogram using facet wrap
ggplot(data = df, aes(x = ValuePerSqFt, fill = Boro)) + 
  geom_histogram(alpha = 0.7) +
  facet_wrap(~Boro)
```
We can see that Brooklyn and Queens make up one mode and Manhattan make up the other. While there is not much data on the Bronx and Staten Island.

Boxplot can also be helpful to analyse the response.

```{r}
#ValuePerSqFt boxplot
ggplot(data = df, aes(x = Boro, y = ValuePerSqFt)) + 
  geom_boxplot(aes(color = Boro))
```

Now we look at histograms for SqFt, Units, Expense, Income and Value.

```{r}
ggplot(data = df, aes(x = Units)) +
  geom_histogram(alpha = 0.7)
```
It seems we have some outliers above 1000 units. 

```{r}
# Check the number of house with more than 1000 units
sum(df$Units >= 1000)
```
We have only 6 outliers. We simply drop them.

```{r}
# Remove outlier based on Units feature
df <- df[df$Units < 1000, ]
```

Now we can analyse over features and check if there are outliers remaining.

```{r}
for (col in c(1:4)) {
  col_name <- names(df)
  g <- ggplot(data = df, aes(x = df[, col])) +
    geom_histogram(alpha = 0.7) +
    xlab(col_name[col])
  print(g)
}
```

They all have a log normal distribution, then it can be difficult for our machine learning model to use these values. Using log give a normal distribution. So we create new columns. 

```{r}
#Create log columns
df$logUnits = log(df$Units)
df$logExpense = log(df$Expense)
df$logIncome = log(df$Income)
df$logSqFt = log(df$SqFt)

head(df)
```
To see how it helps, we can visualize ValuePerSqFt versus log features.

```{r}
#Plot density of postcode per propertyType
for (col in c(7:10)) {
  col_name <- names(df)
  g <- ggplot(data = df, aes(x = df[, col], y = ValuePerSqFt)) +
    geom_point(alpha = 0.7, aes(color = Boro)) +
    xlab(col_name[col])
  print(g)
}

```

```{r}
#Create a correlation matrix
cor_df <- cor(select_if(df, is.numeric))
heatmap(cor_df, scale = "column", margins = c(5,5))
```

### Modeling 

We will define a linear model to predict the value per square foot with other features but without log features in a first step.

```{r}
#Fit the linear regression
model1 <- lm(ValuePerSqFt ~ Boro + Units + SqFt + Expense + Income, 
             data = df)

#Model informations
summary(model1)
```

Now our linear regression are fit. We can use the summary function to print out information about the models.

We have a R-squared of 70%, it reveals that 70% of the variability observed in the target variable (ValuePerSqFt) is explained by the regression model. Which is not bad.

But know using log features instead. 

```{r}
#Fit the linear regression
model2 <- lm(ValuePerSqFt ~ Boro + logUnits + logSqFt + logExpense
             + logIncome, 
             data = df)

#Model informations
summary(model2)
```
The result is much more better, we have a R-squared of 92%, it reveals that 92% of the variability observed in the target variable (ValuePerSqFt) is explained by the regression model. Which is very good


The coefficients represents the effect of the predictors on the response (ValuePerSqFt) and the standard errors are the uncertainty in the estimation of the coefficients. We a visualisation plot to show the coefficient of the regression model. In general, a good rule of thumb is that if the two standard error confidence interval does not contain 0, it is statistically significant.

```{r}
#Visualize model coefficient
multiplot(model1, model2, sort = 'mag')
```

```{r}
#Check model performance with residual against fitted values
ggplot(aes(x = .fitted, y = .resid), data = model2) +
  geom_point(aes(color = factor(Boro)), alpha = 0.7) +
  geom_hline(yintercept = 0) +
  geom_smooth(se = T) +
  labs(x = "Fitted values", y = "Residuals") +
  scale_color_discrete(name = "Boro")

```

```{r}
#Q-Q plot
ggplot(aes(sample = .stdresid), data = model2) + 
  stat_qq() + 
  geom_abline() +
  labs(title = "Q-Q plot")
```

We can create new columns to ensure our result. This is call feature engineering.
Using SqFt feature and Income, Expense and Units features, we can create UnitPerSqft, etc.

```{r}

```

