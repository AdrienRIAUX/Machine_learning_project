---
title: "Projet d'apprentissage statistique"
author: "Adrien Riaux"
date: "18/01/2022"
output: rmarkdown::github_document
---

# Motivation et positionnement du projet

J'ai choisi le sujet concernant la rétinopathie diabétique. Maladie résultant de la complication d'un diabète et de l'atteinte des vaisseaux de la rétine, mettant en jeu un pronostic visuel. C'est une maladie compliquée à traiter et qui a de graves conséquences. C'est pourquoi l'analyse de ce dataset, ainsi que la création d'un modèle prédictif sont très intéressants pour moi. 

De plus le _diabetic retinopathy debrecen dataset_ nous fournit un nombre suffisant d'individu et de varible pour mener à bien cette analyse (ce qui devrait nous éviter d'être en underfitting ou overfitting). Il semblerait qu'il n'y ait pas de valeurs manquantes dans le dataset (même si nous allons le vérifier par la suite). 

La tâche à réaliser pour cette analyse est une classification, puisqu'il faut séparer les cas des patients sains de ceux malades. 

L'ensemble des données contient des caractéristiques extraites des images _Messidor_ dans le but de prédire si une personne est atteinte de rétinopathie diabétique. Toutes les caractéristiques représentent soit une lésion détectée, soit une caractéristique descriptive d'une partie anatomique, soit un descripteur au niveau de l'image. 

# Analyse descriptive 

Les informations qui nous sont fournies sur le dataset sont les suivantes : 

V1) Le résultat binaire de l'évaluation de la qualité. 0 = mauvaise qualité 1 = qualité suffisante.

V2) Le résultat binaire du pré-dépistage, où 1 indique une anomalie rétinienne sévère et 0 son absence.

V3-V8) Les résultats de la détection MA. Chaque valeur de caractéristique représente le nombre d'AM trouvées aux niveaux de confiance alpha = 0,5, . . . , 1, respectivement.

V9-V16) Contiennent les mêmes informations que les variables 3-8, mais pour les exsudats. Comme les exsudats sont représentés par un ensemble de points plutôt que le nombre de pixels construisant les lésions, ces caractéristiques sont normalisées en divisant le nombre de lésions avec le diamètre de la ROI pour compenser des images de différentes tailles.

V17) La distance euclidienne du centre de la macula et le centre du disque optique pour fournir des informations importantes concernant l'état du patient. Cette fonctionnalité est également normalisée avec le diamètre de la ROI.

V18) Le diamètre du disque optique.

V19) Le résultat binaire de la classification AM/FM.

V20) Étiquette de classe. 1 = contient des signes de DR (label cumulatif pour les classes Messidor 1, 2, 3), 0 = aucun signe de DR.

Dans un premier temps, nous allons procéder à une analyse descriptive des données. En particulier, la distribution des différentes variables, et des liens ou des dépendances entres celles-ci.

Pour cela, il nous faut commencer par charger nos données dans un dataframe. Ce qui facilite énormément les manipulation.

```{r}
#On récupère nos données depuis le fichier texte
data = read.table("messidor_features.txt", header = FALSE, sep = ",")
head(data)
```
Ce tableau nous permet d'observer notre dataset de manière globale dans un premier temps. Il nous donne un aperçu des valeurs prisent par chaque variable, ainsi que leur type (int, num...).
Les variables 1, 2, 19 et 20 sont des variables de type catégorielles prenant des valeurs entre {0,1}. Pour en être sûr, nous allons vérifier les valeurs uniques de ces variables.

```{r}
#Numéros des colonnes qui semble être catégorielles
vec <- c(1,2,19,20) 
for (i in vec) {
  print(unique(data[,i]))
}
```

Ce résultat nous confirme que nous avons bien 4 variables de type catégorielles. De plus, à l'aide des informations qui nous sont fournis sur le site du dataset, nous savons que la dernière variable est celle du résultat de l'analyse. Elle nous indique si un patient est malade ou sain.

Désormais, nous allons convertir le type de ces 4 variables en un type catégorielle (à savoir en factor pour le langage R). 

```{r}
#Transformation de type int en factor
for (i in vec){
  data[,i] <- as.factor(data[,i])
}

sapply(data, class)
```

Maintenant que nous avons le bon type pour chacune de nos variables, nous pouvons vérifier si nous avons des valeurs nulles. La commande str() nous donne de nouveau des informations sur les valeurs et le type de chaque variable. Sachant que dans la description du dataset, il y a indiqué 1151 observations et aucune valeur manquante. 

```{r}
#On vérifie le type de chaque variables et si il y des valeurs nulles
str(data)
```
Il n'y a pas de valeurs nulles dans notre dataset. De plus, chaque variable possède désormais le bon type. Nous allons pouvoir procéder à l'analyse préliminaire de nos données. 

```{r}
#On applique sur chaque colonne des statistiques de bases (min, max, moyenne, quartile, etc.)
summary(data)
```
La commande summary nous donne des informations statistiques de base par variables (sauf pour les variables catégorielles, ce qui n'aurait pas de sens). 

On remarque dans un premier temps que pour nos variables de type "int" {V3 à V8}, le minimum est toujours de 1. La médiane et la moyenne sont sensiblement identiques aussi, ce qui nous confirme que ces variables concernent toutes des informations proches. Plus particulièrement les résultats de la détection AM. Il peut être intéressant de les analyser à part des autres dans notre dataset. 

On ne retrouve pas ces similarités dans les variables de type "num".

Pour les variables 1 et 2, on peut remarquer que le nombre de 0 est très faible. Ce qui met en valeur le faible nombre d'évaluation de mauvaises qualités. Cela nous indique aussi que la majorité des patients ont eu un résultat positif au pré-dépistage. Alors que pour la variable 20, on peut voir que l'on à peu près le même nombre de patients malades que sains. On peut donc en conclure sur la faible efficacité du pré-dépistage. 

On peut observer le nombre en % de cas malades/sains, afin d'avoir une meilleurs visibilité sur la variable 20.

```{r}
#On visualise le nombre de cas positif et négatif de notre dataset
my_count <- table(data$V20)
pct <- round(my_count/sum(my_count)*100)
lbls <- c("Negative", "Positive")
lbls <- paste(lbls,pct)
lbls <- paste(lbls,"%", sep = "")

pie(my_count, labels = lbls, main = "Répartition des individus malades/sains", col = c("#F26D6D","#41D9D9"))
```
Ici en rouge, les patients négatifs au RD, et en bleu les patients positifs au RD. Ce qui veut dire que les classes sont équilibrées. Cela nous facilitera la tâche lors de la classification supervisée, car nous n’aurons pas à jouer sur les poids des classes pour rééquilibrer des classes disproportionnées. 

Sachant que la variable 20 constitue notre "target", nous allons analyser la répartition des autres variables en fonction de celle-ci. Pour cela, nous utilisons des boxplots. En séparant les cas sains des cas malades.(Il suffit de cliquer sur un des graphiques pour l'afficher en version grande plus bas).

```{r}
library(ggplot2)
#A l'aide de boxplot on regarde la distribution des individus pour chaque variables
for (i in 3:17){
  g <- ggplot(data, aes(x = V20, y = data[,i]))
  print(g + geom_boxplot(aes(color = V20))
  + xlab("Patient sain/malade")
  + ylab(paste("V",i)))
  
}
```
Ces boxplots sont très intéressants à analyser. Car ils nous donnent des informations sur la répartition des valeurs par variable, et en fonction de la variable "target". On peut voir pour les variables 3 à 8 une légère différence de la répartition des valeurs entre un patient sain et un malade. Il semblerait que les patients malades aient des résultats plus élevés à la détection MA. 

On remarque aussi que pour certaines variables (12 à 16), les patients malades semblent plus souvent répertoriés comme des "outliers" (valeurs aberrantes). 

Nous allons donc maintenant nous intéresser d'un peu plus près au variable 3 à 8.

```{r}
#Analyse des variables 3 à 8 
library(GGally)
GGally::ggpairs(data[c(3:8,20)])
```
Ce graphique met en valeur deux points très importants pour les variables 3 à 8 :  

Tout d'abord, elles ont toutes la même distribution. Ce qui semble logique puisqu'elles sont toutes liées à un test commun. 

Ensuite, ces variables ont de très fortes corrélations (positives). Ce qui veut dire qu'il y a de fortes dépendances entre ces variables. 

On peut donc s'intéresser aux corrélations entre toutes les variables de notre dataset. Pour calculer une matrice de corrélation, il nous faut uniquement des valeurs numériques. On peut donc enlever les variables catégorielles de notre dataset. Sachant qu'il serait très peu pertinent de les inclure dans l'analyse, puisqu'elles constituent les résultats d'analyses postérieurs à celles des variables 3 à 18. 

On commence par définir un dataframe ne contenant que les informations que l'on veut traiter. 

```{r}
#Pour avoir la matrice de corrélation entre nos variables, il faut qu'elles soient de type numérique
corr_data <- cor(data[3:18])
ncol(corr_data)
```
On a bien que les 16 colonnes de notre dataset, qui sont de type "numeric".

On peut donc construire notre matrice de corrélation. Pour cela, on construit d'abord un tableau contenant la corrélation entre chaque combinaisons de variables possibles. 

```{r}
#Préparation des données pour avoir la corrélation entre variables
library(reshape2)
library(scales)
melt_data <- reshape2::melt(corr_data, varnames = c("x","y"), value.name = "Correlation" )
melt_data <- melt_data[order(melt_data$Correlation),]
```
Ensuite on peut afficher à l'aide d'une heatmap, notre matrice de corrélation. 

```{r}
#On utilise une heatmap pour mieux visualiser les corrélations
ggplot(melt_data, aes(x = x, y = y)) +
  geom_tile(aes(fill = Correlation)) +
  scale_fill_gradient2(low = muted("yellow"), mid = "white", high = muted("red"), guide = guide_colorbar(ticks = FALSE, barheight = 10), limits = c(-1,1)) +
  theme_minimal() + 
  labs(x = NULL, y = NULL)
```
On retrouve nos fortes corrélations observées auparavant sur les variables 3 à 8. On peut aussi observer des fortes corrélations entre les variables 9 à 16. 

Cependant, on peut remarquer qu'il n'y a pas ou une très faible corrélation entre les variables qui constituent les résultats de la détection MA (V3-V8) et ceux des exsudats (V9-V16). Il est donc important de prendre ces variables en compte lors de notre analyse.

On peut visualiser la matrice de corrélation d'une autre manière, en y ajoutant un dendrogramme. Ce qui peut être intéressant pour voir les couples de variables. 

```{r}
#Corrélation des variables avec ajout d'un dendrogramme sur la heatmap
heatmap(corr_data, scale = "column", margins = c(2,2))
```
On observe 3 groupes, les deux précédemment observés et un autre qui est formé des variables 17 et 18. Ce dendrogramme nous montre aussi qu'il pourrait être intéressant de regarder le nuage de points entre deux variables qui se suivent (par exemple V13 et V14), car il semblerait que dans notre cas, les premiers duos sont réalisé de cette manière par le dendrogramme. 


On peut aussi s'intéresser à la distribution des variables. 

```{r}
#Distribution des variables
for (i in 3:18){
  col <- data[,i]
  dist <- density(col)
  
  hist(col, xlab="",ylab="", main = paste("Histogram of variable", i))
  
  par(new = T) #Définit que l'on travail sur le même graphique
  plot(dist, col = "red", axes=F, xlab="", ylab="", main="")
}
```
On peut remarquer que le groupe V3-V8 suit une même distribution, qu'il en est de même pour le groupe V9-V16 et le groupe V17-V18.

De plus, on peut en conclure que nous ne sommes pas en présence de distribution normale. 

On s'intéresse donc maintenant à une brève analyse des nuages de points entre les variables.

```{r}
#Analyse des nuages de points
for (i in 3:17){
    g <- ggplot(data, aes(x = data[,i], y = data[,i+1]))
    print(g + geom_point(aes(color = V20)) + facet_wrap(~V20)
    + xlab(paste("V",i,sep = ""))
    + ylab(paste("V",i+1,sep = "")))
}

```
Ces graphiques nous aident à voir qu'en observant certains nuages de points, il est aisé de différencier les points concernant une personne malade d'une personne saine. Ce qui nous conforte dans l'idée qu'une classification supervisée peut mener à de bons résultats. 

Nous allons maintenant passer à l'étape suivante du projet.

# Classification non supervisée

Dans cette partie, nous allons supprimer la variable classe de notre dataset. Dans le but d'utiliser trois méthodes vu en cours : KMeans, PAM et CAH. 

Pour chaque méthode, nous déterminerons le bon nombre de profils type, en extraire les profils et les interpréter. 

```{r}
# On commence par extraire la variable target de notre dataset
unsu_data <- data[-20]
head(unsu_data)
```
Maintenant que nous avons notre "unsupervised dataset", nous pouvons appliquer les différents algorithmes cités auparavant.

Nous allons commencer par le KMeans. Mais avant d'utiliser cet algorithme, il nous faut déterminer le nombre de clusters adéquat. Cela dans le but de maximiser nos résultats. 
On utilise donc la méthode du coude. 

Car il y a une composante aléatoire dans le clustering, nous définissons le "seed" pour générer un résultat reproductible.

```{r}
#Methode du coude
#Initialise nos ratio à zéro
ratio_inter <- 0
ratio_intra <- 0
set.seed(30)

#Dans une boucle on test le KMeans pour k clusters
for (k in 1:10){
  resultat <- kmeans(unsu_data, k)
  ratio_inter[k] <- resultat$betweenss/resultat$totss
  ratio_intra[k] <- resultat$tot.withinss/resultat$totss
}

#Affichage
plot(ratio_inter, type="l", col = "red", )
plot(ratio_intra, type="l", col = "blue")
```
Avec les deux graphiques, nous pouvons voir que la région du coude se situe autour de 4. Nous pouvons donc en conclure que 4 clusters est l'idéal pour réaliser notre apprentissage non supervisé. 

Il existe une autre méthode afin de déterminer le nombre de clusters pour notre algorithme KMeans.
Il s'agit de la fonction "FitKMeans", disponible dans la librairie "useful". 

Elle compare essentiellement le rapport de la somme des carrés intra-cluster pour un cluster avec k clusters, en tenant compte du nombre de lignes et de clusters. Si ce nombre est supérieur à 10, cela vaut la peine d'utiliser k+1 clusters.
```{r}
library(useful)
best_km <- FitKMeans(unsu_data, max.clusters = 10, seed = 30)

best_km
```

Nous allons observer sur un graphique cette méthode, afin d'avoir une meilleur compréhension du résultat. 

```{r}
PlotHartigan(best_km)
```
Ce qui nous indique ces résultats, c'est qu'il faut faudrait ajouter des clusters afin de répondre à la métrique d'Hartigan et afin d'avoir un clustering plus précis. 
Cependant le nombre de clusters devenant trop élever, alors qu'initialement il n'y a que deux classes dans le dataset, nous allons rester sur nos précédents résultats. 
De plus, on observe ici un grand changement lors du passage à 4 clusters. Ce qui nous conforte dans l'utilisation de la méthode du coude pour notre démarche. 

On utilise donc l'algorithme du KMeans avec 4 clusters. 

```{r}
#A l'aide de la methode du coude on peut dire que le meilleur k est 4 
set.seed(30)
km_data <- kmeans(unsu_data, centers = 4)
km_data
```
Les informations ci-dessus nous donne accès aux barycentres de chaque variable, l'assignation de chaque individu à son cluster et la précision de notre clustering en regardant le ratio inter. Ce ratio inter est de 71,9%, ce qu'on peut considérer comme un résultat correct.

Car le KMeans a ses limites et il est possible que dans notre dataset, il n'y est pas de groupes distincts qui se dessinent. Afin de vérifier cela nous pouvons afficher graphiquement les résultats de notre clustering. 

Cependant, celui-ci peut être difficile en raison de la nature dimensionnelle élevée des données. Pour surmonter ce problème, la fonction plot effectue une mise à l'échelle multidimensionnelle pour projeter les données en deux dimensions. Elle utilise l'algorithme des PCA, et affiche sur les deux composantes principales les plus importantes.
```{r}
#Graphique du résultat du KMeans 
plot(km_data, data = data, class = "V20")
```
Ce graphique nous aide à voir qu'il semble compliqué de séparer les classes. Et que nos clusters possèdent des individus des deux classes.

Nous pouvons extraire le profil des clusters grâce à une matrice de confusion. 

```{r}
#Matrice de confusion pour notre clustering
plot(table(data$V20, km_data$cluster), main="Matrice de confusion", xlab = "V20", ylab = "Clusters")
```
On remarque que nos clusters 1 et 2 possèdent plus d'individus que les clusters 3 et 4. Le clusters 4 possède peu d'individus négatifs. Il semblerait que ce clusters représente le mieux les individus positifs. Cependant, il faut prendre en compte qu'elle représente un total d'individus assez faible. Et que les clusters 1 et 2 représentes de nombreux individus positifs. 
Nous pouvons donc en conclure qu'il est difficile de reproduire la distinction entre individus malades et sains avec l'algorithme des KMeans.

On peut aussi réaliser un autre KMeans, avec seulement 2 clusters, puisque nous savons que notre variable cible classifie nos patients en deux catégories : malade ou sain. 
On analyse ensuite avec ce même graphique l'efficacité de notre algorithme. 

```{r}
#Test avec deux clusters pour reproduire notre variable "target"
km_data2 <- kmeans(unsu_data, centers = 2)
km_data2
plot(km_data2, data = data, class = "V20")
```
Les résultats obtenus ne sont pas bons, puisque nous avons seulement un résultat de 42,3% pour le ratio inter. De plus, si on regarde le graphique obtenu, on peut voir qu'il y a une faible corrélation entre les couleurs (signification de notre clustering) et les formes (signification des classes réelles), ce qui nous indique un mauvais clustering. 

Afin de vérifier cela, nous utilisons une matrice de confusion.

```{r}
plot(table(data$V20, km_data2$cluster), main="Matrice de confusion", xlab = "V20", ylab = "Clusters")
#La classe 0 correspond à la 1 et la 1 à la 2 pour la variable V20
```
Cette matrice nous montre que notre cluster 1 correspond trop peu d'élément de la classe 0 (équivalent au cluster 1). Si les résultats étaient bons, la diagonale de la matrice aurait de fortes valeurs.

L'un des problèmes pour le clustering avec l'algorithme KMeans est qui est sensible au outliers (valeurs aberrantes). Et lors de notre analyse descriptive, nous avons grâce aux boxplots, que notre jeu de donné possède des outliers. 


Une alternative à ce problème est les K-Medoïds. Au lieu que le centre d'un cluster soit la moyenne du cluster, le centre est l'une des observations réelles du cluster. Cela s'apparente à la médiane, qui est robuste contre les valeurs aberrantes. L'algorithme le plus connu des K-Medoïds est Partitionning Around Medoids (PAM). 

```{r}
#On importe la librairie permettant d'utiliser PAM
library(cluster)
#On passe nos data dans la matrice de distance entre les individus
X <- dist(unsu_data)
#Utilisation de l'algorithme PAM sur la matrice de distance
#On garde le même nombre de cluster que pour le kmeans
pam_data <- pam(X, k = 4)
pam_data
```
On obtient le résultat ci-dessus, nous nous intéressons au résultat du build et du swap de la fonction objective. On remarque, que nos résultats ne s'améliorent que très peu après la deuxième étape de l'algorithme (le swap). 

On va maintenant observer la silhouette de notre clustering, ce qui nous donnera plus d'information sur l'efficacité de notre algorithme. 
```{r}
#Afin d'afficher graphiquement le résultat, on utilise la fonction silhoutte
sil <- silhouette(pam_data)
#On met le paramètre border à NA afin d'éviter des problèmes d'affichage du graphique
plot(sil, main = "", border = NA)
```
Nous savons que plus l'average width est élevée, plus notre clustering est bon. Or, dans notre cas, elle est de 0.33. Un consensus scientifique autour de la valeur silhouette nous dit qu'une valeur de 0.33 correspond à une structure faible, presque une structure artificielle.  

Il nous faut donc utiliser une autre méthode. Il est possible que nos classes ne soient pas linéairement séparables. 

On utilise donc une dernière méthode de clustering. La classification hiérarchique ascendante (CAH). 
Cette méthode construit des clusters dans des clusters. Elle ne requière pas d'un nombre initial de clusters. Cette méthode peut être vue comme un arbre, représenté graphiquement par un dendrogramme.  

```{r}
#On utilise la fonction hclust pour utiliser l'algorithme de la CAH
hc_data <- hclust(X, method = "ward.D")
plot(hc_data, labels = FALSE)
#Séparation en 4 clusters
rect.hclust(hc_data, k = 4, border = "red")
#Séparation en 8 clusters
rect.hclust(hc_data, k = 8, border = "blue")
```
On observe qu'avec 4 clusters, chacun d'entre eux possèdent un poids élevé. Afin de réaliser un test nous avons aussi regardé avec 8 clusters. Le poids semble être divisé par deux, mais pour cela, nous avons dû doubler le nombre de clusters. Cela n'est pas donc pas intéressant dans notre cas.
Nos 4 clusters ont un poids très élevé, ce qui ne permet pas de dire que nous avons un bon clustering. Peut importe la méthode nos résultats sont mauvais.

On en déduit que la classification non supervisé ne marche pas. Cela peut s'expliquer par le fait que certaines variables n'ont pas de rapport avec le phénomène observé (ici la variable V20). Ces variables biaisent donc nos résultats. 

Cependant, cela ne sera pas le cas avec un arbre de décision, qui lui regarde variable par variable. Ce qui nous amène au point suivant. 

# Classification supervisée

Dans cette partie, nous allons procéder à l'apprentissage d'un arbre de classification pour la prédiction de la variable classe en fonction des variables restantes. 

Nous allons utiliser un protocole d'apprentissage par Booststrap, indiquer les performances de l'arbre appris, visualiser et interpréter l'arbre induit en analysant les principales règles de décision. 

On relie nos données. 

```{r}
#On récupère nos données depuis le fichier texte
data = read.table("messidor_features.txt", header = FALSE, sep = ",")
#On définit la varibale 20 comme une variable catégorielle
data$V20 = as.factor(data$V20)
head(data)
```
Maintenant que nous avons récupéré nos données, nous pouvons réaliser notre apprentissage. 

Pour réaliser un apprentissage supervisé, nous allons séparer notre dataset en un trainset et un testset, dans le but de vérifier les résultats de notre algorithme.

```{r}
#Définition de pourcentage de données alloué à l'entraînement
smp_size <- floor(0.8 * nrow(data))

#On donne seed pour obtenir tout le temps la même partition
set.seed(12)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)

#Split en trainset et testset
trainset <- data[train_ind, ]
testset <- data[-train_ind, ]
```

Maintenant que nous avons notre testset et trainset, nous allons préparer nos dataframes pour les utiliser dans un algortihme d'arbre de décision et xgboost. 

Nous allons commencer par un algorithm de decision tree.

```{r}
#Import des librairies utiles pour faire un arbre
library(rpart)
library(rpart.plot)
#Définition de l'arbre et affichage de celui-ci
fit <- rpart(trainset$V20~., data = trainset)
rpart.plot(fit)
```
Grâce à cet arbre, nous pouvons visualiser les principales règles. On peut voir sur chaque feuille la classe majoritaire de la feuille (0 ou 1), ainsi que le pourcentage d'individus total de la feuille. On remarque que nous avons 7 règles de décisions, et que nous avons donc 8 feuilles à notre arbre. Dont 5 correspondants à la classe 1 (individu malade) et 3 correspondants à la classe 0 (individu sain). 

Une autre visualisation est possible, moins complète, mais qui met plus en valeur les règles de l'arbre et les classes obtenues suite a ces règles.

```{r}
#On peut réaliser une autre méthode d'affichage
plot(fit)
text(fit)
```
De plus, les règles sont données avec une plus grande précision après la virgule. 
On peut maintenant afficher notre arbre sans utiliser de graphique, afin d'avoir plus d'information sur les règles obtenus. 

```{r}
#Ici on affiche les règles de notre arbre, la classe majoritaire de chaque régles
fit
```
On remarque que sur les feuilles obtenues sur les 920 individus utilisés pour notre apprentissage, certaines ont très bien classées les individus, et la classe majoritiaire de la feuille est supérieur à 70%. Alors que certaines feuilles ont moins bien classés les individus.

Cet affichage nous montre : la règle, le nombre d'individus de la classe majoritaire, puis le nombre d'individus de la classe minoritaire. 
Enfin nous avons la probabilité d'appartenance à une classe. Par exemple pour la règle 3 qui nous dit que la variable V3 doit être supérieur à 55.5, nous avons une probabilité de 78% d'être dans la classe 1. 

On peut maintenant s'intéresser à la performance de notre arbre. Pour cela nous utilisons une matrice de confusion.

```{r}
#Pour calculer la performance de notre arbre nous allons utiliser une matrice de confusion
library(caret)
ypred <- predict(fit, testset, type = "class")
result <- confusionMatrix(ypred, testset$V20)
result
```
On observe que nous avons une précision de 65%, ce qui n'est pas très élevé. On peut donc utiliser une méthode par boostrap afin d'améliorer nos résulats. 

```{r}
#On définie nos vecteurs de variables train et test et nos vecteurs de variable target train et test
#On convertie nos dataframe en matrice car xgboost ne prend en entrée que des matrices
X_train <- as.matrix(trainset[-20])
y_train <- as.matrix(trainset[20])
X_test <- as.matrix(testset[-20])
y_test <- as.matrix(testset[20])
```

Maintenant que nous avons nos matrices, contenant nos variables dans une première matrice et notre variables target dans une seconde. Nous pouvons utiliser l'algorithme xgboost afin de réaliser un arbre de décision par boostrap. 

On règle les paramètres suivants : eta contrôle le taux d'apprentissage, il échelonne la contribution de chaque arbre. Utilisé pour éviter l'overfitting en rendant le processus de boosting plus conservateur. Une valeur plus faible pour eta implique une valeur plus grande pour nrounds. Une valeur eta faible signifie un modèle plus robuste au surajustement mais plus lent à calculer. nrounds nous permet de fixer le nombre d'itération que l'algorithme fait. Enfin max.depth nous permet de contrôler la profondeur maximale de l'arbre. 

En jouant sur ces hyperparamètres, nous pouvons améliorer les performances de notre algorithme. 

```{r}
#XGBOOST
library(xgboost)
boostedtree <- xgboost(data = X_train, label = y_train, eta = .3, nrounds = 25, max.depth = 8, objective = 'binary:logistic')
```

On utilisant l'algorithme de xgboost, nous arrivons au bout de 25 itérations à grandement améliorer les résultats de notre apprentissage. Nous arrivons à une erreur moyenne de 12% seulement. Ce qui représente de bons résultats. 

On peut donc maintenant regarder les variables les plus importantes lors de l'apprentissage de notre arbre. Afin de comprendre quelles variables sont les plus utiles dans l'apprentissage de l'arbre. 
```{r}
#Affichage de l'importance des variables
library(DiagrammeR)
xgb.plot.importance(xgb.importance(boostedtree, feature_names = colnames(X_train)))
```
On voit que la variable 3 est la plus importante. La variable 15 et 9 le sont aussi. 

```{r}
#Prediction du modèle sur le jeu de test
y_pred <- predict(boostedtree, X_test, type = "class")

#Convertion du vecteur de probabilité en classe 1 ou 0
y_pred <- as.numeric(y_pred > 0.5)

#Erreur moyenne sur le jeu de test
err <- mean(y_pred != y_test)
print(paste("test-error=", err))
```
```{r}
#Matrice de confusion
result <- confusionMatrix(factor(y_pred), factor(y_test))
result
```
On peut donc voir qu'avec XgBoost, on améliore nos résultats de presque 10%, cependant il faut faire attention à l'overfitting de notre modèle. 

Lors de notre apprentissage supervisé, nous avons dû utiliser une méthode par bosstrap afin d'améliorer nos performances. Celle-ci est très utilisée et permet d'obtenir simplement des très bons résultats (comme nous avons pu le voir). Elle est plus efficace qu'un simple arbre de décision, de plus pour un problème de simple classification comme celui-ci, il n'est pas nécessaire de se lancer dans des algorithmes de deep learning. 

# Conclusion 

Pour conclure, nous avons dans ce projet une approche complète pour traiter un jeu de données. En procédant dans un premier temps par une analyse statistique des données, afin de mieux les comprendre, et de mieux aborder les étapes suivantes du projet. Nous n'avons pratiquement pas eu de pré-traitement des données à faire, ce qui m'a permis de me concentrer sur les aspects vus en cours. 
Dans un deuxième temps, nous avons abordé le sujet du clustering, en supprimant notre variable target, et en utilisant des algorithmes d'apprentissages non supervisés comme KMeans, PAM et CAH. Malheureusement les résultats n'ont pas été très bons. Le KMeans nous a apporté le meilleur résultat, même si celui-ci n'est pas tout à fait convaincant, sachant que le KMeans est sensible aux outliers, tester PAM pouvait être une solution. Cependant les résultats sont moindres avec cet algorithme. Nous en avons conclu que les classes sont non linéairement séparables. Ce qui rend la tâche d'apprentissage non supervisé très compliqué. Une ouverture sur ce problème, est le deep learning où des méthodes commencent à émerger. 
Enfin, dans un troisième temps, nous avons réalisé une classification supervisée. Avec tout d'abord un arbre de décision, afin de voir les règles obtenues par notre arbre, ainsi que la répartition de chaque classe dans les feuilles obtenues. Les résultats n'étant pas suffisamment efficaces, nous avons procédé à une méthode par boostrap. Sur laquelle nous avons obtenu de meilleurs résultats (89% de précision avec 20 itérations). 
Ce projet m'a permis de mettre en application les concepts vus en cours et de me familiariser avec le langage R, un langage nouveau pour moi.
